{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yewb6nAb6974",
        "outputId": "3161d423-d4d6-4df1-e44c-f1020fe40cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting webvtt-py\n",
            "  Downloading webvtt_py-0.5.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading webvtt_py-0.5.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: webvtt-py\n",
            "Successfully installed webvtt-py-0.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pandas matplotlib nltk webvtt-py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KhfYvQGAqyX",
        "outputId": "6b953e9d-9eef-4e3f-d83a-e3df44441531"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "Oar3wJMCAyeU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJCYiqLDC1hY",
        "outputId": "b465bd13-ae0f-4aac-e023-a143869086b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWVjv-G_K8su",
        "outputId": "e8ca73d4-89ff-41d8-9b03-d0d6207c1cf3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. Parsing Comments into a DataFrame"
      ],
      "metadata": {
        "id": "igFrYNd9XQEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def structure_comments_from_txt(filepath='/content/drive/MyDrive/CSE477/comment.txt'):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    comments_data = []\n",
        "    time_regex = re.compile(r'.*(second|minute|hour|day|week|month|year)s? ago.*', re.IGNORECASE)\n",
        "    i = 0\n",
        "    while i < len(lines):\n",
        "        line = lines[i].strip()\n",
        "        if i + 1 < len(lines) and time_regex.match(lines[i+1].strip()):\n",
        "            username = line\n",
        "            timestamp = lines[i+1].strip()\n",
        "            comment_text = []\n",
        "            i += 2\n",
        "            while i < len(lines) and not (i+1 < len(lines) and time_regex.match(lines[i+1].strip())):\n",
        "                comment_line = lines[i].strip()\n",
        "                if comment_line and not comment_line.lower() in ['reply', '...more']:\n",
        "                    comment_text.append(comment_line)\n",
        "                i += 1\n",
        "            if comment_text:\n",
        "                comments_data.append({\n",
        "                    'username': username,\n",
        "                    'timestamp_text': timestamp,\n",
        "                    'comment_text': ' '.join(comment_text)\n",
        "                })\n",
        "        else:\n",
        "            i += 1\n",
        "    return pd.DataFrame(comments_data)\n",
        "\n",
        "# Run the function and check your results!\n",
        "comments_df = structure_comments_from_txt()\n",
        "print(comments_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYN91cBjQE3C",
        "outputId": "caa5f7cb-6cd3-4c9c-999b-40aa6da3b66e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 username        timestamp_text  \\\n",
            "0    @risebyliftingothers  2 years ago (edited)   \n",
            "1  @auliamardhatillah2240           2 years ago   \n",
            "2             @limwei2634           2 years ago   \n",
            "3         @nancykataria08          2 months ago   \n",
            "4                @jpbaugh           2 years ago   \n",
            "\n",
            "                                        comment_text  \n",
            "0  ₹100.00 Thanks for an amazingly simplified app...  \n",
            "1  Yesterday I click on a video called 'learning ...  \n",
            "2  I've been trying to learn ML for quite awhile ...  \n",
            "3  No fancy words, just simple English and the ri...  \n",
            "4  For anyone getting an error related to convert...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(comments_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAu1oE-qRWxM",
        "outputId": "d2abb7fb-9fdd-47ae-a77d-0f5c627dd80a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   username        timestamp_text  \\\n",
            "0      @risebyliftingothers  2 years ago (edited)   \n",
            "1    @auliamardhatillah2240           2 years ago   \n",
            "2               @limwei2634           2 years ago   \n",
            "3           @nancykataria08          2 months ago   \n",
            "4                  @jpbaugh           2 years ago   \n",
            "..                      ...                   ...   \n",
            "195          @rickgomez7480           2 years ago   \n",
            "196                  @wqwng           2 years ago   \n",
            "197                @tvdeath           2 years ago   \n",
            "198          @GrandZangoule            1 year ago   \n",
            "199      @nqobilelerato5751            1 year ago   \n",
            "\n",
            "                                          comment_text  \n",
            "0    ₹100.00 Thanks for an amazingly simplified app...  \n",
            "1    Yesterday I click on a video called 'learning ...  \n",
            "2    I've been trying to learn ML for quite awhile ...  \n",
            "3    No fancy words, just simple English and the ri...  \n",
            "4    For anyone getting an error related to convert...  \n",
            "..                                                 ...  \n",
            "195                                      $5.00 Thanks!  \n",
            "196                                      $2.00 Thanks!  \n",
            "197                                    €5.00 Thanks! 2  \n",
            "198                                     $20.00 Thanks!  \n",
            "199                                    CA$2.79 Thanks!  \n",
            "\n",
            "[200 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Parsing Captions into a DataFrame"
      ],
      "metadata": {
        "id": "0IZUZkyJXZao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import webvtt\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def structure_captions_from_vtt(filepath):\n",
        "    try:\n",
        "        full_text = ' '.join([caption.text.strip() for caption in webvtt.read(filepath)])\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading VTT file: {e}. Falling back to manual line reading.\")\n",
        "        captions = []\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if '-->' not in line and line and not line.isdigit() and 'WEBVTT' not in line:\n",
        "                    captions.append(line)\n",
        "        full_text = ' '.join(captions)\n",
        "    sentences = re.split(r'(?<=[.!?]) +', full_text)\n",
        "    return pd.DataFrame(sentences, columns=['caption_sentence'])\n",
        "\n",
        "captions_df = structure_captions_from_vtt('/content/drive/MyDrive/CSE477/Caption.vtt')\n",
        "print(captions_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UTMKZOTTA2H",
        "outputId": "36be21a0-f9f4-4b4f-a71e-972b4003f322"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    caption_sentence\n",
            "0  kylie ying has worked at many kylie ying has w...\n",
            "1             so let's actually just accuracy is 81.\n",
            "2             so let's actually just accuracy is 81.\n",
            "3  so let's actually just\\nmake this five make th...\n",
            "4                  of people that have covid is 531.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(captions_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oj177TpsTL0N",
        "outputId": "aee8a0ba-fdee-4ba9-a86f-e4a744eb25de"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     caption_sentence\n",
            "0   kylie ying has worked at many kylie ying has w...\n",
            "1              so let's actually just accuracy is 81.\n",
            "2              so let's actually just accuracy is 81.\n",
            "3   so let's actually just\\nmake this five make th...\n",
            "4                   of people that have covid is 531.\n",
            "5   of people that have covid is 531.\\nso i'm goin...\n",
            "6                                                  1.\n",
            "7   1.\\nso here let's rewrite this so here let's r...\n",
            "8                                this over this is 1.\n",
            "9   this over this is 1.\\nso now my probability is...\n",
            "10                             equal to let's say 32.\n",
            "11  equal to let's say 32.\\nall right um all right...\n",
            "12             so now it'd be a row of 64 this to 64.\n",
            "13             so now it'd be a row of 64 this to 64.\n",
            "14  so now it'd be a row of 64\\nnodes and then 32 ...\n",
            "15            let's say let's keep the epochs at 100.\n",
            "16  let's say let's keep the epochs at 100.\\nand n...\n",
            "17            actually let's get rid of throw in 128.\n",
            "18            actually let's get rid of throw in 128.\n",
            "19                     actually let's get rid of\\n16.\n",
            "20                                          sorry 16.\n",
            "21                                          sorry 16.\n",
            "22  sorry\\nlet's throw 128 in there that should be...\n",
            "23                      0.001 and a batch size of 64.\n",
            "24  0.001 and a batch size of 64.\\nand it does see...\n",
            "25                             so if i set that to 1.\n",
            "26                             so if i set that to 1.\n",
            "27  so if i\\nactually i think what happens if i do...\n",
            "28                                         this to 8.\n",
            "29  this to 8.\\nokay okay okay\\nso i don't really ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C. The Cleaning Pipeline"
      ],
      "metadata": {
        "id": "Gf1zpQujXesW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer"
      ],
      "metadata": {
        "id": "cNbSypmcW5ed"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "def stem_tokens(tokens):\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "def clean_text_pipeline(text, use_lemmatization=True):\n",
        "    normalized = normalize_text(text)\n",
        "    tokens = tokenize_text(normalized)\n",
        "    filtered = remove_stopwords(tokens)\n",
        "    if use_lemmatization:\n",
        "        final_tokens = lemmatize_tokens(filtered)\n",
        "    else:\n",
        "        final_tokens = stem_tokens(filtered)\n",
        "    return final_tokens\n",
        "\n",
        "# Example application:\n",
        "comments_df['cleaned_tokens'] = comments_df['comment_text'].apply(lambda x: clean_text_pipeline(x, use_lemmatization=True))\n",
        "captions_df['cleaned_tokens'] = captions_df['caption_sentence'].apply(lambda x: clean_text_pipeline(x, use_lemmatization=True))\n"
      ],
      "metadata": {
        "id": "p7nA5zbVW8xM"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df.to_csv('/content/drive/MyDrive/CSE477/comments_output.csv', index=False)"
      ],
      "metadata": {
        "id": "mkgZf9HvSIB5"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions_df.to_csv('/content/drive/MyDrive/CSE477/captions_output.csv', index=False)"
      ],
      "metadata": {
        "id": "M8VcJPU7TkYc"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}